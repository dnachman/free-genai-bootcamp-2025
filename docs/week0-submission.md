## Hypothesis and technical uncertainties

Evaluating the prompting approach across several AI powered assistants. It's my assumption that the paid assistants will provide better output than open source ones (such as ollama). I'm also curious to see how the paid and free tier assistants compare to each other. I've also been using models locally and am curious to see how they fair against the cloud based models.

## Technical exploration

I followed along with the meta-ai model that Andrew used in the video and worked through some of the prompts. Not sure all of them were relevant.

I also explored the Chatgpt model (Paid) and it was interesting to see the various outputs from a "normal" transformer (4o) and the reasoning transformer (o3-mini).

Lastly, I explored the Ollama model (Free) and it was interesting to see the output from those models as well. At one point Mistral just gave the complete response in Spanish which was not helpful

I purposely asked for conjugation examples for more extensive help because that's something I remember having trouble with in Spanish class in High School (~ 30 years ago).

## Final Outcomes

The meta-ai model and the paid ChatGPT 4o models seem to have done the best at following instructions and making it interactive. The reasoning models didn't quite understand the interactivity that I was seeking. Some of the other open source models fell far short.

ChatGPT is probably the best for a paid option I have access to (I don't currently subscribe to others). For open source, llama3.2:2b did a fine job.

## Anything else you'd like to add

Glad to be in another of your bootcamps! The submission example process video is really helpful. It would be great to have a concise "expectations" video/section at the beginning of each week to help guide the work, especially if we want to figure it out on our own.

I've had ollama + webui running on a moderately old (3 yrs) PC (11 gen i7, RTX 2060 Super 8gb) and just upgraded to using M4 Pro MacBook Pro w/ 24 gb.
